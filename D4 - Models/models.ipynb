{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7DaysOfCode - Machine Learning \n",
    "\n",
    "## Day-2: Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing is a crucial step in the Machine Learning workflow as it ensures the quality of our data. This step involves cleaning, organizing, and transforming raw data into high-quality data that can be used to train models. Properly preprocessed data directly impacts the ability of our models to learn and make accurate predictions.\n",
    "\n",
    "Some of the data preprocessing steps, such as handling null values and duplicates, were already addressed earlier in this documentation. In this step, I will focus on enriching the dataset, standardization, and checking for multicollinearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_df = pd.read_csv(\"C:/Users/SAMSUNG/OneDrive/Documentos/GitHub/7DaysOfCode_SpotifyML/data/day_one.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89740, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the binary class"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I am dealing with a binary classification problem, where the goal is to predict whether a song will be popular or not, I need to encode the 'popularity' column as the target variable. I have chosen a threshold of 70, which means that songs with a popularity score above 70 will be considered popular and encoded as 1, while those with a score below this threshold will be considered not popular and encoded as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target value: popularity\n",
    "spotify_df['popularity'] = spotify_df['popularity'].apply(lambda x: 1 if x > 70 else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Feature engineering is the process of creating new featurs or transforming existing features in a dataset to improve the perfomance of machine learning models. In this step I will remove irrelevant columns and create new features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5SuOikwiRyPMVoIQDJUgSV</td>\n",
       "      <td>Gen Hoshino</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>1</td>\n",
       "      <td>3.844433</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4qPNDBW1i3p13qLCt0Ki3A</td>\n",
       "      <td>Ben Woodward</td>\n",
       "      <td>Ghost (Acoustic)</td>\n",
       "      <td>Ghost - Acoustic</td>\n",
       "      <td>0</td>\n",
       "      <td>2.493500</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1iJBSr7s7jYXzM8EGcbK5b</td>\n",
       "      <td>Ingrid Michaelson;ZAYN</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>To Begin Again</td>\n",
       "      <td>0</td>\n",
       "      <td>3.513767</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6lfxq3CG4xtTiEg7opyCyx</td>\n",
       "      <td>Kina Grannis</td>\n",
       "      <td>Crazy Rich Asians (Original Motion Picture Sou...</td>\n",
       "      <td>Can't Help Falling In Love</td>\n",
       "      <td>1</td>\n",
       "      <td>3.365550</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5vjLSffimiIP26QG5WcN2K</td>\n",
       "      <td>Chord Overstreet</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>Hold On</td>\n",
       "      <td>1</td>\n",
       "      <td>3.314217</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 track_id                 artists  \\\n",
       "0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
       "1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
       "2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
       "3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
       "4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
       "\n",
       "                                          album_name  \\\n",
       "0                                             Comedy   \n",
       "1                                   Ghost (Acoustic)   \n",
       "2                                     To Begin Again   \n",
       "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
       "4                                            Hold On   \n",
       "\n",
       "                   track_name  popularity  duration_ms  explicit  \\\n",
       "0                      Comedy           1     3.844433     False   \n",
       "1            Ghost - Acoustic           0     2.493500     False   \n",
       "2              To Begin Again           0     3.513767     False   \n",
       "3  Can't Help Falling In Love           1     3.365550     False   \n",
       "4                     Hold On           1     3.314217     False   \n",
       "\n",
       "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
       "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
       "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
       "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
       "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
       "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
       "\n",
       "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
       "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
       "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
       "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
       "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
       "4          0.000000    0.0829    0.167  119.949               4    acoustic  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this initial stage,  I will be working exclusively with numeric data in this dataset. I have chosen to exclude categorical variables for some reasons:\n",
    "\n",
    "i. Limited contribution to model performance: Variables such as track_id, album_name, and track_name are unlikely to significantly improve the performance of the model, as they may not contain meaningful numerical information for the model to learn from.\n",
    "\n",
    "ii. High cardinality of variables: Variables like genre and artist may have a large number of unique categories, which can result in increased computational overhead and additional data cleaning steps. Considering the potential gains in model performance, dealing with these variables may be time-consuming and cumbersome.\n",
    "\n",
    "By excluding these categorical variables, I can streamline the data preparation process and focus on working with numerical data, which may be more relevant for the specific modeling task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_df = spotify_df.drop(columns = ['track_id', 'album_name', 'track_name', 'artists', 'track_genre'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.844433</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.493500</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3.513767</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3.365550</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3.314217</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.1670</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3.570667</td>\n",
       "      <td>False</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.4810</td>\n",
       "      <td>6</td>\n",
       "      <td>-8.807</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.2890</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>98.017</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3.823333</td>\n",
       "      <td>False</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>2</td>\n",
       "      <td>-8.822</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0355</td>\n",
       "      <td>0.8570</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>0.0765</td>\n",
       "      <td>141.284</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>4.049100</td>\n",
       "      <td>False</td>\n",
       "      <td>0.703</td>\n",
       "      <td>0.4440</td>\n",
       "      <td>11</td>\n",
       "      <td>-9.331</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.5590</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.7120</td>\n",
       "      <td>150.960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>3.160217</td>\n",
       "      <td>False</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.4140</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.700</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.2940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>0.6690</td>\n",
       "      <td>130.088</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>3.426567</td>\n",
       "      <td>False</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.6320</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.770</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>0.4260</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0.0735</td>\n",
       "      <td>0.1960</td>\n",
       "      <td>78.899</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   popularity  duration_ms  explicit  danceability  energy  key  loudness  \\\n",
       "0           1     3.844433     False         0.676  0.4610    1    -6.746   \n",
       "1           0     2.493500     False         0.420  0.1660    1   -17.235   \n",
       "2           0     3.513767     False         0.438  0.3590    0    -9.734   \n",
       "3           1     3.365550     False         0.266  0.0596    0   -18.515   \n",
       "4           1     3.314217     False         0.618  0.4430    2    -9.681   \n",
       "5           0     3.570667     False         0.688  0.4810    6    -8.807   \n",
       "6           1     3.823333     False         0.407  0.1470    2    -8.822   \n",
       "7           1     4.049100     False         0.703  0.4440   11    -9.331   \n",
       "8           1     3.160217     False         0.625  0.4140    0    -8.700   \n",
       "9           0     3.426567     False         0.442  0.6320    1    -6.770   \n",
       "\n",
       "   mode  speechiness  acousticness  instrumentalness  liveness  valence  \\\n",
       "0     0       0.1430        0.0322          0.000001    0.3580   0.7150   \n",
       "1     1       0.0763        0.9240          0.000006    0.1010   0.2670   \n",
       "2     1       0.0557        0.2100          0.000000    0.1170   0.1200   \n",
       "3     1       0.0363        0.9050          0.000071    0.1320   0.1430   \n",
       "4     1       0.0526        0.4690          0.000000    0.0829   0.1670   \n",
       "5     1       0.1050        0.2890          0.000000    0.1890   0.6660   \n",
       "6     1       0.0355        0.8570          0.000003    0.0913   0.0765   \n",
       "7     1       0.0417        0.5590          0.000000    0.0973   0.7120   \n",
       "8     1       0.0369        0.2940          0.000000    0.1510   0.6690   \n",
       "9     1       0.0295        0.4260          0.004190    0.0735   0.1960   \n",
       "\n",
       "     tempo  time_signature  \n",
       "0   87.917               4  \n",
       "1   77.489               4  \n",
       "2   76.332               4  \n",
       "3  181.740               3  \n",
       "4  119.949               4  \n",
       "5   98.017               4  \n",
       "6  141.284               3  \n",
       "7  150.960               4  \n",
       "8  130.088               4  \n",
       "9   78.899               4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot enconding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handling categorical data is a critical step in the machine learning workflow. Even though I have excluded categorical data with high cardinality, there are still some remaining variables that need to be addressed. Certain machine learning models are unable to work with categorical data directly, and it's important to avoid misinterpretation of categorical variables as continuous ones, such as interpreting key as an integer variable when it is actually a categorical variable. To address this, I will be utilizing One Hot Encoding to appropriately represent these categorical variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(df, column, prefix):\n",
    "    df = df.copy()\n",
    "    dummies = pd.get_dummies(df[column], prefix=prefix, drop_first=True) # Drop the first column to avoid multicollinearity\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    df = df.drop(column, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the dumies\n",
    "spotify_df = onehot_encode(spotify_df, 'mode', 'mode')\n",
    "spotify_df = onehot_encode(spotify_df, 'explicit', 'isExplicit')\n",
    "spotify_df = onehot_encode(spotify_df, 'key', 'key')\n",
    "spotify_df = onehot_encode(spotify_df, 'time_signature', 'time_signature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89740, 28)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>mode_1</th>\n",
       "      <th>isExplicit_True</th>\n",
       "      <th>key_1</th>\n",
       "      <th>key_2</th>\n",
       "      <th>key_3</th>\n",
       "      <th>key_4</th>\n",
       "      <th>key_5</th>\n",
       "      <th>key_6</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>key_11</th>\n",
       "      <th>time_signature_1</th>\n",
       "      <th>time_signature_3</th>\n",
       "      <th>time_signature_4</th>\n",
       "      <th>time_signature_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3.844433</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.715</td>\n",
       "      <td>87.917</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.493500</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.267</td>\n",
       "      <td>77.489</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3.513767</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.120</td>\n",
       "      <td>76.332</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3.365550</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.143</td>\n",
       "      <td>181.740</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3.314217</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.167</td>\n",
       "      <td>119.949</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   popularity  duration_ms  danceability  energy  loudness  speechiness  \\\n",
       "0           1     3.844433         0.676  0.4610    -6.746       0.1430   \n",
       "1           0     2.493500         0.420  0.1660   -17.235       0.0763   \n",
       "2           0     3.513767         0.438  0.3590    -9.734       0.0557   \n",
       "3           1     3.365550         0.266  0.0596   -18.515       0.0363   \n",
       "4           1     3.314217         0.618  0.4430    -9.681       0.0526   \n",
       "\n",
       "   acousticness  instrumentalness  liveness  valence    tempo  mode_1  \\\n",
       "0        0.0322          0.000001    0.3580    0.715   87.917       0   \n",
       "1        0.9240          0.000006    0.1010    0.267   77.489       1   \n",
       "2        0.2100          0.000000    0.1170    0.120   76.332       1   \n",
       "3        0.9050          0.000071    0.1320    0.143  181.740       1   \n",
       "4        0.4690          0.000000    0.0829    0.167  119.949       1   \n",
       "\n",
       "   isExplicit_True  key_1  key_2  key_3  key_4  key_5  key_6  key_7  key_8  \\\n",
       "0                0      1      0      0      0      0      0      0      0   \n",
       "1                0      1      0      0      0      0      0      0      0   \n",
       "2                0      0      0      0      0      0      0      0      0   \n",
       "3                0      0      0      0      0      0      0      0      0   \n",
       "4                0      0      1      0      0      0      0      0      0   \n",
       "\n",
       "   key_9  key_10  key_11  time_signature_1  time_signature_3  \\\n",
       "0      0       0       0                 0                 0   \n",
       "1      0       0       0                 0                 0   \n",
       "2      0       0       0                 0                 0   \n",
       "3      0       0       0                 0                 1   \n",
       "4      0       0       0                 0                 0   \n",
       "\n",
       "   time_signature_4  time_signature_5  \n",
       "0                 1                 0  \n",
       "1                 1                 0  \n",
       "2                 1                 0  \n",
       "3                 0                 0  \n",
       "4                 1                 0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Scaling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, I will perform standardization. Standardization involves transforming the values of the dataset such that the mean of the values is 0 and the standard deviation is 1. This is crucial because many machine learning models, including those that use the gradient descent algorithm, are sensitive to the scale of the input features. Having columns with different scales can result in issues such as overshooting or loss of computational efficiency. Standardization helps to mitigate these problems and ensures that the data is normalized to a common scale, making it more suitable for machine learning model training and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['popularity', 'duration_ms', 'danceability', 'energy', 'loudness',\n",
       "       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
       "       'valence', 'tempo', 'mode_1', 'isExplicit_True', 'key_1', 'key_2',\n",
       "       'key_3', 'key_4', 'key_5', 'key_6', 'key_7', 'key_8', 'key_9', 'key_10',\n",
       "       'key_11', 'time_signature_1', 'time_signature_3', 'time_signature_4',\n",
       "       'time_signature_5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spotify_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = spotify_df.loc[:, 'popularity']\n",
    "X = spotify_df.drop('popularity', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardization\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day-3: Train Test Validation\n",
    "\n",
    "### Train-test sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, my focus is on splitting the dataset into train, test, and validation sets. In machine learning, we split the data into train and test sets to allow our model to learn from the patterns in the training data. Once the learning is done, the model can then make predictions on unseen data from the test set. This helps us evaluate how well the model can generalize to new, unseen data and assess its performance.This step is important to avoid issues with overfitting or underfitting, which can negatively impact the model's ability to generalize to new data and result in lower accuracy.\n",
    "\n",
    "A common practice is to allocate around 80% of the data for training and 20% for testing. This is the approach I will be using to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89740\n",
      "89740\n"
     ]
    }
   ],
   "source": [
    "print(len(X_std))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.969545\n",
       "1    0.030455\n",
       "Name: popularity, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state = 42, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential drawback of simply splitting the dataset into train/test sets is that the splitting may not be random, which can lead to issues with overfitting. To address this concern, I will be performing a cross-validation step. Cross-validation involves creating multiple train/test sets by splitting the data into k subsets, and training the model on k-1 of these subsets. There are various cross-validation methods available, and for this exercise, I will be using the StratifiedKFold method.\n",
    "\n",
    "The `StratifiedKFold` method is particularly useful as it ensures that the class proportions within each fold are as close as possible to the real class proportions in the overall dataset. By using cross-validation, we can obtain a more robust assessment of our model's performance and ensure that it can generalize well to unseen data, improving its reliability for real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of StratifiedKFold with the desired number of splits\n",
    "skf = StratifiedKFold(n_splits= 5, shuffle= True, random_state= 42)\n",
    "\n",
    "# Iterate over the splits and perform cross-validation\n",
    "for train_index, val_index in skf.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.reset_index(drop=True)[train_index], y_train.reset_index(drop=True)[val_index]  ## .reset_index is to ensure data alignment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The `.reset_index`method in pandas is used to ensure proper alignment of data. It is typically used to reset the index of a dataframe, which can be useful in cases where the index is not sequential or needs to be re-aligned after performing certain operations on the data. This step can help avoid issues such as bugs caused by misaligned data (which I encountered in my code). By using.`reset_index`, we can ensure that the data is in the correct order and alignment, which is essential for accurate analysis and modeling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-4: Modeling and Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baselines: Dummy Classifier and Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy Classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have split the data, I will be working on some baseline models. A baseline model serves as a starting point for performance comparison with other models. It is usually a simple model that allows us to establish a benchmark. If a model performs better than the baseline model, then it can be considered a good model. Additionally, a baseline model can be useful in identifying potential issues with bias or variance. For example, if a baseline model performs poorly, with low accuracy, it may indicate problems with bias or variance in the data.\n",
    "\n",
    "I will start with a **dummy classifier**. A dummy classifier is so named because it does not use the data to make predictions. Instead, it uses a pre-defined strategy that we pass to it. In this case, I will set the strategy to 'most frequent', which predicts the most frequent class in the dataset. In our dataset, the most frequent class is '0', as there are more music items that fall under the label of 'not popular' (scoring less than 70 on the popularity score) than those that are popular."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy classifier provides a null accuracy baseline, which is the accuracy that can be achieved by always predicting the most frequent class. While a dummy classifier is not typically used in real-world classification problems, it can be useful in certain scenarios. If the models I'm going to train perform close to the performance of the dummy classifier, it may indicate that the features in the model are ineffective, erroneously computed, or missing for some reason. It could also indicate a large class imbalance, where the accuracy gains on the test set simply applied to too few examples to produce a significant gain. In such cases, I will need to work on solutions for the imbalanced class and consider other metrics besides accuracy. Therefore, dummy classifiers can provide a useful sanity check and point of comparison for evaluating model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_model = DummyClassifier(strategy = 'most_frequent' )\n",
    "\n",
    "#training the model\n",
    "dummy_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "#predicting on the train data\n",
    "dummy_pred_train = dummy_model.predict(X_train_fold)\n",
    "\n",
    "#predicting on the validation data\n",
    "dummy_pred_val = dummy_model.predict(X_val_fold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on dummy classifier set on train data \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Prediction on dummy classifier set on validation data \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediction on dummy classifier set on train data \\n', dummy_pred_train[:200])\n",
    "print(f'Prediction on dummy classifier set on validation data \\n', dummy_pred_val[:200])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you can read the result from DummyClassifier's predictions. It's always predicting 0 or the negative class for every instance in the test set.  Now we can call the usual score method to get the accuracy of the DummyClassifier's constant negative prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on the training set:  0.9691820176202249\n",
      "accuracy on the validation set:  0.9691461206296141\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy on the training set: ',dummy_model.score(X_train_fold, y_train_fold))\n",
    "print(f'accuracy on the validation set: ',dummy_model.score(X_val_fold, y_val_fold))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy score of 0.96 is very high, but it should be noted that this dataset is imbalanced, so such a high accuracy is expected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can be seen as a kind of generalized linear model. Like ordinary least squares and other regression methods, logistic regression takes a set of input variables (features) and estimates a target value. However, unlike ordinary linear regression, the target value in logistic regression is binary, typically represented as 0 or 1. The logistic regression model uses a logistic function to transform the input from a real value to an output Y that is bounded between 0 and 1. This output is interpreted as the probability of the object belonging to the positive class, given the input characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression model\n",
    "log_model = LogisticRegression()\n",
    "\n",
    "# training the model\n",
    "log_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "# predicting on the train set\n",
    "log_pred_train = log_model.predict(X_train_fold)\n",
    "\n",
    "# predicting on the validation set\n",
    "log_pred_val = log_model.predict(X_val_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds = np.exp(log_model.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>loudness</th>\n",
       "      <td>1.879194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>danceability</th>\n",
       "      <td>1.232885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isExplicit_True</th>\n",
       "      <td>1.211023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_8</th>\n",
       "      <td>1.090023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_1</th>\n",
       "      <td>1.074752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_11</th>\n",
       "      <td>1.051540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_9</th>\n",
       "      <td>1.033019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_3</th>\n",
       "      <td>1.030284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_5</th>\n",
       "      <td>1.026017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_10</th>\n",
       "      <td>1.024640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_4</th>\n",
       "      <td>1.022513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_6</th>\n",
       "      <td>1.020652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>valence</th>\n",
       "      <td>0.987762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_2</th>\n",
       "      <td>0.984284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mode_1</th>\n",
       "      <td>0.972700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_7</th>\n",
       "      <td>0.959818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>duration_ms</th>\n",
       "      <td>0.952894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempo</th>\n",
       "      <td>0.939759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speechiness</th>\n",
       "      <td>0.804861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_signature_5</th>\n",
       "      <td>0.783366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_signature_1</th>\n",
       "      <td>0.758144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liveness</th>\n",
       "      <td>0.758096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_signature_3</th>\n",
       "      <td>0.714035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time_signature_4</th>\n",
       "      <td>0.684397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acousticness</th>\n",
       "      <td>0.665290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>energy</th>\n",
       "      <td>0.598361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instrumentalness</th>\n",
       "      <td>0.450752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      coef\n",
       "loudness          1.879194\n",
       "danceability      1.232885\n",
       "isExplicit_True   1.211023\n",
       "key_8             1.090023\n",
       "key_1             1.074752\n",
       "key_11            1.051540\n",
       "key_9             1.033019\n",
       "key_3             1.030284\n",
       "key_5             1.026017\n",
       "key_10            1.024640\n",
       "key_4             1.022513\n",
       "key_6             1.020652\n",
       "valence           0.987762\n",
       "key_2             0.984284\n",
       "mode_1            0.972700\n",
       "key_7             0.959818\n",
       "duration_ms       0.952894\n",
       "tempo             0.939759\n",
       "speechiness       0.804861\n",
       "time_signature_5  0.783366\n",
       "time_signature_1  0.758144\n",
       "liveness          0.758096\n",
       "time_signature_3  0.714035\n",
       "time_signature_4  0.684397\n",
       "acousticness      0.665290\n",
       "energy            0.598361\n",
       "instrumentalness  0.450752"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(log_odds,\n",
    "             X.columns,\n",
    "             columns=['coef']).sort_values(by='coef', ascending = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression models can be more challenging to interpret due to the use of the logit function, which returns probabilities. To make sense of these probabilities, we often convert them to odds by taking the exponential of the logs. For example, when the `loudness` variable increases by one unit, the odds of the music being in the target class (\"1\") are over 1.8 times as large as the odds of it not being in the target class. On the other hand, when the `instrumentalness` variable increases by one unit, the odds of the music being in the target class are only around 0.4. Since the odds are less than one, we can take the reciprocal (1/odds) to gain better insight. Thus, as instrumentalness increases by 1, the odds of the music **NOT** being in the target class are 2.2 times as likely as the odds of it being in the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.9691820176202249\n",
      "Accuracy on validation set: 0.9691461206296141\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy on the train set\n",
    "accuracy_train = log_model.score(X_train_fold, y_train_fold)\n",
    "print(\"Accuracy on train set:\", accuracy_train)\n",
    "\n",
    "# Calculate accuracy on the validation set\n",
    "accuracy_val = log_model.score(X_val_fold, y_val_fold)\n",
    "print(\"Accuracy on validation set:\", accuracy_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon calculating only the accuracy, we can observe that the Logistic Regression model, as it stands, is not performing better than the Dummy Classifier. This suggests that we may have issues with an imbalanced dataset. To improve the generalization of the model, we will need to address this issue by applying methods such as resampling techniques to mitigate the impact of class imbalance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
